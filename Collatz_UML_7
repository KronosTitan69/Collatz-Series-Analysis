import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt

def collatz_steps(n):
    steps = 0
    while n != 1:
        if n % 2 == 0:
            n //= 2
        else:
            n = 3*n + 1
        steps += 1
    return steps

# Generate dataset
max_n = 100000
X = np.arange(2, max_n)
y = np.array([collatz_steps(n) for n in X])

# Feature engineering: log(n), parity, bit count
log_n = np.log(X)
parity = X % 2
bit_count = [bin(n).count("1") for n in X]
X_features = np.vstack([X, log_n, parity, bit_count]).T

# ML Model
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=42)
rf = RandomForestRegressor(n_estimators=100, max_depth=10)
rf.fit(X_train, y_train)

# Predict and evaluate
y_pred = rf.predict(X_test)
print("MAE:", mean_absolute_error(y_test, y_pred))

# Monte Carlo: prediction intervals
def monte_carlo_predict(model, X_sample, n_runs=100):
    preds = []
    for _ in range(n_runs):
        noise = np.random.normal(0, 1, X_sample.shape)
        pred = model.predict(X_sample + noise)
        preds.append(pred)
    preds = np.array(preds)
    mean_pred = preds.mean(axis=0)
    std_pred = preds.std(axis=0)
    return mean_pred, std_pred

mean_pred, std_pred = monte_carlo_predict(rf, X_test)
plt.scatter(X_test[:,0], y_test, alpha=0.3, label="True")
plt.scatter(X_test[:,0], mean_pred, alpha=0.3, label="Predicted")
plt.fill_between(X_test[:,0], mean_pred-std_pred, mean_pred+std_pred, alpha=0.2, color='orange', label="MC Interval")
plt.xlabel("n")
plt.ylabel("Collatz Steps")
plt.legend()
plt.show()